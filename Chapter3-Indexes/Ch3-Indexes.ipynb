{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Indexes\n",
    "This notebook has the code I used to perform the classification experiments on Scottish Enlightenment philosophical histories. There are three different tests that I describe below. The unique aspect of this chapter is that I tried the classifier on the word categories from the Linguistic Inquiry and Word Count dictionary as way to examine the distinctive approach to history found in the Scottish texts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import bookFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpora and organize corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I describe three different classification tests in Chapter 3. \n",
    "1. Systematic vs. historical texts\n",
    "2. Philosophical histories vs. narrative histories\n",
    "3. Indexes for philosophical histories vs. indexes for narrative histories\n",
    "\n",
    "This experiment uses word categories from the [*Linguistic Inquiry and Word Count 2015*](https://liwc.app/), which is proprietary software. You'll need to purchase it or obtain it through your institution to use. There is a 2022 version of the word category dictionary now. The datasets below included texts that have already been transformed from word lists to LIWC word categories. If you'd like to perform the tokenization and word category transformation, there is another notebook for doing that.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scottish Enlightenment Corpus\n",
    "This is the full Scottish Enlightenment that I use in the chapter. It is stored as a list of dictionaries with keys for the metadata and the text, which is stored as a word list. Here you can examine the texts prior to being transformed into lists of word categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scottishCorpus = pickle.load(open('scottishCorpus.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the fields for the corpus.\n",
    "print(scottishCorpus[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic versus Historical Scottish Enlightenment Texts\n",
    "This test set is organized as a list of tuples: `(text class, liwc word categories lists)`. The classes are `systematic` and `historical`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysHistCorpus = pickle.load(open('sysVsHistCorp.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Philosophical histories versus Narrative Histories\n",
    "The following dataset is organized as a list of tuples with a each item like this `(text class, liwc word category list)`. The text classes are `philhist` and `sample`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyWordCats = pickle.load(open('philHistSampCats.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Philosophical History indexes versus narrative history indexes\n",
    "For the index analysis, I have the texts and metadata for the sample historical indexes, and the full set of philosophical history and sample index word categories in `indexCorpusWordCats`. Above the test set I also include `sampleIndexCorpus`, which is a list of dictionaries including the full texts and metadata for all the sample indexes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleIndexCorpus = pickle.load(open('eccoSampIndex.p', 'rb')) ### These are the indexes used in comparison with the philosophical history indexes. \n",
    "indexCorpusWordCats = pickle.load(open('philHistIndexCatsCorp.p', 'rb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize the datasets \n",
    "Here you need to update the `sysHistCorpus` variable to perform whichever group you want to perform the classification test with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sysHistCorpus ## This is the corpus for the first test. Replace it if you want to perform the second or third test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [] \n",
    "wordCatChunkList = [] \n",
    "for item in corpus:\n",
    "    catList = item[1].split()\n",
    "    splitText = bookFunctions.splitText(catList, 500) ## You can tinker with the chunk size\n",
    "    for chunk in splitText:\n",
    "        wordCatChunkList.append(chunk)\n",
    "        categories.append(item[0])\n",
    "print(len(wordCatChunkList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'category': categories, 'text': wordCatChunkList}\n",
    "dfFull = pd.DataFrame(d)\n",
    "dfFull['category_id'] = dfFull['category'].factorize()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word categories vectors\n",
    "Create term vectors from the word category chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.process_time() \n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,1), stop_words=None, min_df=.10, norm='l2', use_idf=False, \n",
    "                       max_features=60)\n",
    "feats = tfidf.fit_transform(dfFull.text).toarray()\n",
    "vocabulary = tfidf.get_feature_names()\n",
    "labels = dfFull.category_id\n",
    "print(time.process_time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train four different classifiers to compare their degrees of accuracy before selecting one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0, multi_class='ovr', solver='liblinear',),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, feats, labels, scoring='accuracy', cv=CV) \n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "mod = LinearSVC() \n",
    "## Below are other models that you might try. \n",
    "#clf = MultinomialNB()\n",
    "#rf = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "#logR = LogisticRegression(random_state=0, multi_class='ovr', solver='lbfgs')\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(feats, labels, dfFull.index, \n",
    "                                                                                 test_size=0.33, random_state=0)\n",
    "mod.fit(X_train, y_train)\n",
    "y_pred = mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear SVC\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names=dfFull['category'].unique())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the classifier results \n",
    "These functions create a dataframe for sorting the most informative LIWC word categories for distinguishing the two groups. You need to update the text class label to change it from the first classifier test to the second or third. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modTerms = bookFunctions.mostInformTerms(mod, topn=30)\n",
    "dfTM = bookFunctions.docTermMatrix (feats, tfidf)\n",
    "scotMeanDF, sampMeanDF = bookFunctions.buildMeanDF(dfTM, feats, 'systematic', categories) ## Update the label here.\n",
    "scotWords, sampWords = bookFunctions.topGroupWord (scotMeanDF, sampMeanDF, modTerms, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### phil hist set\n",
    "print(sorted(scotWords, key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sample 2\n",
    "print(sorted(sampWords, key=lambda x:x[1], reverse=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
