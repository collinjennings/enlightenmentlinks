{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 - Epigraphs \n",
    "\n",
    "This is the notebook analyzing how different paratextual headers correlate with stylistic differences between British novels of the 1790s. As in chapter two, I compare the object in question (here, novels with chapter epigraphs) to other contemporary forms (epistolary novels and ones with chapter summaries) using a cluster analysis experiment and a classification one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required libararies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import time\n",
    "import numpy as np \n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import pylab as pl\n",
    "import bookFunctions\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load novel corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCorpus = pickle.load(open('works-fullNovelCorpus.pickle', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Observe structure of the corpus\n",
    "fullCorpus[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [] \n",
    "paratexts = [] \n",
    "authors = [] \n",
    "titles = [] \n",
    "for i in fullCorpus:\n",
    "    ### Uncomment the next three lines if you don't want to include the bill-of-fare novels.\n",
    "    #if i['paratext'] == 'bill-of-fare': \n",
    "       # continue\n",
    "    #else: \n",
    "    texts.append(i['text'])\n",
    "    paratexts.append(i['paratext'])\n",
    "    titles.append(i['title'])\n",
    "print(len(texts))\n",
    "print(len(paratexts))\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(paratexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View rest of the metadata for the novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/collinjennings/Dropbox/eccoDriveWork/metadata/novelCorpusESTCMeta.txt\") as f:\n",
    "    a = [{k.strip(): v.strip() for k, v in row.items() if v is not None  } for row in csv.DictReader(f, delimiter=';')]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of first row\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the terms of the novels and visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.process_time() \n",
    "vectorizer = TfidfVectorizer(input='content', decode_error='replace', ngram_range=(3,3), stop_words=None,\n",
    "                             max_features=200, min_df=.20, norm='l2', use_idf=True)\n",
    "dtm = vectorizer.fit_transform(texts)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dtm2 = dtm.toarray()\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(dtm)\n",
    "\n",
    "print(len(vocab))\n",
    "print(time.process_time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce the number of dimensions with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(dist)\n",
    "pos = pca.transform(dist)\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe and visualize the PCA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(x=xs, y=ys,title=titles, label=paratexts)) \n",
    "#group by cluster\n",
    "groups = df.groupby('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with just the epistolary and epigraph novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idList = [38, 43, 31, 78, 95, 140, 178, 203, 214, 222, 223, 229, 219, 231, \n",
    "         42, 32, 84, 92] \n",
    "cluster_colors = {'epigraphs': '#EBEBEB', 'bill-of-fare': '#D9D9D9', 'epistolary': '#BBBABA'} \n",
    "fig, ax = plt.subplots(figsize=(12, 12)) # set size\n",
    "ax.margins(0.05)\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=14, \n",
    "            label=name, color=cluster_colors[name]) \n",
    "    ax.legend(numpoints=1) \n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    if i in idList: \n",
    "        ax.text(df.loc[i]['x'], df.loc[i]['y'], df.loc[i]['title'], ## maybe iloc\n",
    "            horizontalalignment='center', weight='bold', size=10)  \n",
    "plt.title('1790s Epigraph, Bill-of-Fare, and Epistolary Novels Clustered by Top 400 Trigrams with TSNE', fontsize=16)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with bill-of-fare novels included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot\n",
    "cluster_colors = {'epigraphs': '#F6F6F6', 'bill-of-fare': '#D9D9D9', 'epistolary': '#BBBABA'} \n",
    "fig, ax = plt.subplots(figsize=(8, 8)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=14, \n",
    "            label=name, color=cluster_colors[name]) #, ## removed cluster_name dict \n",
    "            #mec='none')\n",
    "  \n",
    "    ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    if i in idList: \n",
    "        ax.text(df.loc[i]['x'], df.loc[i]['y'], df.loc[i]['title'], ## maybe iloc\n",
    "            horizontalalignment='center', weight='bold', size=10)  \n",
    "#plt.title('1790s Epigraph, Bill-of-Fare, and Epistolary Novels Clustered by Top 400 Trigrams with TSNE', fontsize=16)\n",
    "plt.savefig('epigraphViz/newLabelBillEpiEpis.jpg', bbox_inches='tight', dpi=400)\n",
    "plt.show() #show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify shared features of different clusters in the graph\n",
    "Set the limits for each axis below and then indicate which side of the limit you want to examine--to the left or right of the x-value and above or below the y-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLimit = None\n",
    "yLimit = None\n",
    "aboveVal = True\n",
    "ToTheLeftVal = True\n",
    "\n",
    "posGroup, notGroup = bookFunctions.separator(pos, xLimit, yLimit, aboveVal,  ToTheLeftVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgArray, notAvgArray = bookFunctions.analyzer(dtm2, posGroup, notGroup)\n",
    "gothFull, notGothFull = bookFunctions.valueRanker(graph_analyzer.differencer\n",
    "                                                             (avgArray, notAvgArray),  vocab, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(gothFull, key=lambda x:x[1], reverse=True))\n",
    "print()\n",
    "print(sorted(notGothFull, key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Organize the corpus for classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraCheck = ['epigraphs', 'epistolary', 'bill-of-fare']\n",
    "texts = [] \n",
    "paratexts = [] \n",
    "authors = [] \n",
    "titles = [] \n",
    "for item in fullCorpus:\n",
    "    if item['paratext'] in paraCheck and 'text' in item.keys():\n",
    "        paratexts.append(item['paratext'])\n",
    "        texts.append(item['text'])\n",
    "        authors.append(item['author'])\n",
    "        titles.append(item['title']) \n",
    "\n",
    "titles2 = [title[:15] for title in titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dataframe for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'paratext': paratexts, 'title':titles2, 'text': texts}\n",
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_id'] = df['paratext'].factorize()[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_df = df[['paratext', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'paratext']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.process_time() \n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, ngram_range=(2,3), stop_words=None, min_df=.50, norm='l2', use_idf=True) ### CountVectorizer\n",
    "features = tfidf.fit_transform(df.text).toarray()\n",
    "labels = df.category_id\n",
    "print(time.process_time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0, multi_class='multinomial', solver='lbfgs'),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV) \n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a classifier and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names=df['paratext'].unique())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the classifier decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(X_train)\n",
    "pca_2d = pca.transform(X_train)\n",
    "plt.figure(figsize=(10,10))\n",
    "svmClassifier_2d = LinearSVC().fit(pca_2d, y_train)\n",
    "for idx, i in enumerate(y_train.keys()):\n",
    "    if y_train[i] == 0:\n",
    "        c1 = plt.scatter(pca_2d[idx,0],pca_2d[idx,1],c='r',    s=50,marker='+')\n",
    "    elif y_train[i] == 1:\n",
    "        c2 = plt.scatter(pca_2d[idx,0],pca_2d[idx,1],c='g',    s=50,marker='o')\n",
    "    elif y_train[i] == 2:\n",
    "        c3 = plt.scatter(pca_2d[idx,0],pca_2d[idx,1],c='b',    s=50,marker='*')\n",
    "plt.legend([c1, c2, c3], ['Bill-of-Fare', 'Epistolary',   'Epigraph'])\n",
    "x_min, x_max = pca_2d[:, 0].min() -.5,   pca_2d[:,0].max()+.5   # remove -1, +1\n",
    "y_min, y_max = pca_2d[:, 1].min()-.5 ,   pca_2d[:, 1].max()+.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .001),   np.arange(y_min, y_max, .001))\n",
    "Z = svmClassifier_2d.predict(np.c_[xx.ravel(),  yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, alpha=0.8)\n",
    "plt.title('Support Vector Machine Decision Surface')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
